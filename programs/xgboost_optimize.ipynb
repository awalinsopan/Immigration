{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.time import Time\n",
    "from scipy import interpolate\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plot\n",
    "from geopy.distance import distance as gdistance\n",
    "from sklearn.preprocessing import StandardScaler as SCALER\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import immigration_data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate,GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import immigration_data\n",
    "import importlib\n",
    "importlib.reload(immigration_data)\n",
    "imm = immigration_data.immigration_data()\n",
    "X,y,d = imm.get_training_data()\n",
    "year_pred = imm.year_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can not use standard cross validation because spatial correlations lead to data leakage. Have to separate train and test sets separating by years. The routine below does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_immigration_data(reg, X, y, year_pred):\n",
    "\n",
    "    ind_train1 = (year_pred != 2005) & (year_pred != 2006)\n",
    "    ind_test1  = (year_pred == 2005) | (year_pred == 2006)    \n",
    "    ind_train2 = (year_pred != 2007) & (year_pred != 2008)\n",
    "    ind_test2  = (year_pred == 2007) | (year_pred == 2008)\n",
    "    ind_train3 = (year_pred != 2009) & (year_pred != 2010)\n",
    "    ind_test3  = (year_pred == 2009) | (year_pred == 2010)\n",
    "    ind_train4 = (year_pred != 2011) & (year_pred != 2012)\n",
    "    ind_test4  = (year_pred == 2011) | (year_pred == 2012)\n",
    "    ind_train5 = (year_pred != 2013) & (year_pred != 2014)\n",
    "    ind_test5  = (year_pred == 2013) | (year_pred == 2014)\n",
    "    ind_train6 = (year_pred != 2015) & (year_pred != 2016) & (year_pred != 2017)\n",
    "    ind_test6  = (year_pred == 2015) | (year_pred == 2016) | (year_pred == 2017)\n",
    "    \n",
    "    Xtrain1 = X[ind_train1] \n",
    "    Xtrain2 = X[ind_train2]\n",
    "    Xtrain3 = X[ind_train3]\n",
    "    Xtrain4 = X[ind_train4]\n",
    "    Xtrain5 = X[ind_train5]\n",
    "    Xtrain6 = X[ind_train6]\n",
    "\n",
    "    Xtest1 = X[ind_test1]\n",
    "    Xtest2 = X[ind_test2]\n",
    "    Xtest3 = X[ind_test3]\n",
    "    Xtest4 = X[ind_test4]\n",
    "    Xtest5 = X[ind_test5]\n",
    "    Xtest6 = X[ind_test6]\n",
    "    \n",
    "    ytrain1 = y[ind_train1]\n",
    "    ytrain2 = y[ind_train2]\n",
    "    ytrain3 = y[ind_train3]\n",
    "    ytrain4 = y[ind_train4]\n",
    "    ytrain5 = y[ind_train5]\n",
    "    ytrain6 = y[ind_train6]\n",
    "\n",
    "    ytest1 = y[ind_test1]\n",
    "    ytest2 = y[ind_test2]\n",
    "    ytest3 = y[ind_test3]\n",
    "    ytest4 = y[ind_test4]\n",
    "    ytest5 = y[ind_test5]\n",
    "    ytest6 = y[ind_test6]\n",
    "    \n",
    "    reg.fit(Xtrain1, ytrain1)\n",
    "    ypred1 = reg.predict(Xtest1)\n",
    "    #score1 = mean_absolute_error(ytest1, ypred1)\n",
    "    score1 = r2_score(ytest1, ypred1)\n",
    "\n",
    "    reg.fit(Xtrain2, ytrain2)\n",
    "    ypred2 = reg.predict(Xtest2)\n",
    "    score2 = r2_score(ytest2, ypred2)\n",
    "\n",
    "    reg.fit(Xtrain3, ytrain3)\n",
    "    ypred3 = reg.predict(Xtest3)\n",
    "    score3 = r2_score(ytest3, ypred3)\n",
    "  \n",
    "    reg.fit(Xtrain4, ytrain4)\n",
    "    ypred4 = reg.predict(Xtest4)\n",
    "    score4 = r2_score(ytest4, ypred4)\n",
    "\n",
    "    reg.fit(Xtrain5, ytrain5)\n",
    "    ypred5 = reg.predict(Xtest5)\n",
    "    score5 = r2_score(ytest5, ypred5)\n",
    "\n",
    "    reg.fit(Xtrain5, ytrain5)\n",
    "    ypred5 = reg.predict(Xtest5)\n",
    "    score5 = r2_score(ytest5, ypred5)\n",
    "\n",
    "    reg.fit(Xtrain6, ytrain6)\n",
    "    ypred6 = reg.predict(Xtest6)\n",
    "    score6 = r2_score(ytest6, ypred6)\n",
    "    \n",
    "    score_out = np.median([score1, score2, score3, score4, score5, score6])\n",
    "    \n",
    "    return score_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization procedure is iterative. The approach is to fit all the features, do hyper parameter optimization, remove the least important feature, calcualte score, redo hyper parameter optimization, remove least important feature etc etc. I only do 20 evaluations for each iteration due to computing limitations. Ideally, the number of evaluations per iteration would be larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "\n",
    "def get_best_indices(reg_trained, X, y, year_pred):\n",
    "\n",
    "    thresholds = np.sort(reg_trained.feature_importances_)\n",
    "    feature_importance = reg_trained.feature_importances_\n",
    "\n",
    "    master_index = np.arange(len(X[0,:]))\n",
    "\n",
    "    score_out = []\n",
    "    index_out = []\n",
    "    \n",
    "    nindex = len(master_index)\n",
    "    X_in = X.copy()\n",
    "    \n",
    "    while nindex > 2:\n",
    "        index = np.where((feature_importance > thresholds[0]) & (feature_importance > 0))[0]\n",
    "        el = len(index)\n",
    "        \n",
    "        X_in = X_in[:,index]\n",
    "        master_index = master_index[index]\n",
    "        index_out.append(master_index)\n",
    "        # Run hyperopt optimization\n",
    "\n",
    "        def objective(para):\n",
    "            # print(para['max_depth'],para['learning_rate'])\n",
    "            reg = xgb.XGBRegressor(**para,objective='reg:squarederror')\n",
    "\n",
    "            testScore = CV_immigration_data(reg, X_in, y, year_pred)\n",
    "            #print(testScore)\n",
    "            return {'loss': -1 * testScore, 'status': STATUS_OK}\n",
    "\n",
    "        \n",
    "        trials = Trials()\n",
    "        space = {\n",
    "            'learning_rate':    hp.choice('learning_rate',    np.arange(0.1, 0.2, 0.02)),\n",
    "            'max_depth':        hp.choice('max_depth',        np.arange(6, 10, 1, dtype=int)),\n",
    "            'min_child_weight': hp.choice('min_child_weight', np.arange(4, 8, 1, dtype=int)),\n",
    "            'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "            'subsample':        hp.uniform('subsample', 0.75, 0.95),\n",
    "            'n_estimators':     hp.choice('n_estimators',      np.arange(20,100,10))   }\n",
    "        result = fmin(fn = objective, space = space, algo = tpe.suggest,\n",
    "                      trials=trials, max_evals=20)\n",
    "        \n",
    "        reg_trained = get_best_xgb_model.get_best_xgb_model(space_eval(space, result))\n",
    "        reg_trained.fit(X_in, y)\n",
    "        feature_importance = reg_trained.feature_importances_\n",
    "        thresholds = np.sort(reg_trained.feature_importances_)\n",
    "\n",
    "        \n",
    "        score = CV_immigration_data(reg_trained, X_in , y, year_pred)\n",
    "        print('N Features = ' ,el , 'R2 score = ', score)\n",
    "        score_out.append(score)\n",
    "        \n",
    "        nindex = len(index)\n",
    "\n",
    "    return score_out, index_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the best score and the corresponding indices. For this particular run, the best set of indices was at iteration 80. Rerunning this with larger number of evaluations may produce different results and should be tested. After finding the best set of indices, run a higher number of evaluations to find the best set of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_sel = index_out[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [3:50:01<00:00, 37.33s/it, best loss: -0.1923667965474231]   \n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "def objective(para):\n",
    "    # print(para['max_depth'],para['learning_rate'])\n",
    "    reg = xgb.XGBRegressor(**para,objective='reg:squarederror')\n",
    "\n",
    "    testScore = CV_immigration_data(reg, X[:, index_sel], y, year_pred)\n",
    "    #print(testScore)\n",
    "    return {'loss': -1 * testScore, 'status': STATUS_OK}\n",
    "                                \n",
    "\n",
    "# Run hyperopt optimization\n",
    "trials = Trials()\n",
    "space = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.06, 0.2, 0.02)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(4, 14, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(2, 12, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.75, 0.99),\n",
    "    'n_estimators':     hp.choice('n_estimators',      np.arange(20,100,5))   }\n",
    "result = fmin(fn = objective, space = space, algo = tpe.suggest,\n",
    "              trials=trials, max_evals=300)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para is the best set of hyper parameters. I took these from this hyper parameter/feature selection optimization sequence and hard coded them into the get_best_xgb_model.py routine. If rerunning the hyper parameter optimization changes the result, modify that routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 8,\n",
       " 'n_estimators': 65,\n",
       " 'subsample': 0.9174674408083486}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = space_eval(space, result)\n",
    "print(para)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
